{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5, Machine Learning 2022\n",
    "\n",
    "The following lab-session is adapted from those of Sections 5.3 and 6.3 in Introduction to Statistical Learning with R.\n",
    "\n",
    "### The Auto dataset\n",
    "We use the `Auto` dataset that was used as an example throughout Chapter 3 on linear regression. Here, we treat the variable `mpg` (gas miles in miles per gallon) as the response and `horsepower` as the single predictor.\n",
    "\n",
    "The data has 392 observations on the following 9 variables.\n",
    "\n",
    "`mpg` miles per gallon\n",
    "\n",
    "`cylinders` Number of cylinders between 4 and 8 displacementEngine displacement (cu. inches) \n",
    "\n",
    "`horsepower` Engine horsepower\n",
    "\n",
    "`weight` Vehicle weight (lbs.)\n",
    "\n",
    "`acceleration` Time to accelerate from 0 to 60 mph (sec.)\n",
    "\n",
    "`year` Model year (modulo 100)\n",
    "\n",
    "`origin` Origin of car (1. American, 2. European, 3. Japanese)\n",
    "\n",
    "`name` Vehicle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame\n",
    "from math import log, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0         130    3504          12.0    70   \n",
       "1  15.0          8         350.0         165    3693          11.5    70   \n",
       "2  18.0          8         318.0         150    3436          11.0    70   \n",
       "3  16.0          8         304.0         150    3433          12.0    70   \n",
       "4  17.0          8         302.0         140    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv; change the directory as you need!\n",
    "Auto = read_csv(\"Auto.csv\")\n",
    "# show the first line of the dataset to see if everything was alright\n",
    "Auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is recommended that you set a random seed, so that any results based on randomness are recreated whenever you run the notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WHY DO WE NEED A SEED?*\n",
    "\n",
    "Basically now we will split our dataset into training and test data or maybe into three so training validation and test data :\n",
    "\n",
    "                                    |train|validation|   or   |train|validation|test| \n",
    "\n",
    "In order to randomly generate this subsets we want to create a random  number generator that will divide our full data into this subsets. What we also wana be able to do is to alaways retrieve the same set otherwise it would be impossible to make comparison etc... In order to do this we give our random number generator a seed which gives the ability of retrieving the same sequence every time\n",
    "\n",
    "written by Lamberto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set seed!\n",
    "# np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation set approach\n",
    "\n",
    "Split the set of observations into two halves by selecting a random subset of 196 obervations out of the original 392 observations. We refer to these observations as the *training set* and the remaining observations as the *validation set*. For creating the random split, you can use `random.sample(seq, n)` to select `n` numbers without replacement from the sequence `seq`. (There is also a built-in function `train_test_split` that can split your data, but here you split it yourself so that you understand in detail how to achieve the datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we are gonna do here is split our data into set and training data \n",
    "# explanation on the markdown below\n",
    "\n",
    "def randomize_split(n, n_samples):\n",
    "\n",
    "    train = random.sample([x for x in range(n)], n_samples)\n",
    "\n",
    "    validation = [x for x in range(n) if x not in train]\n",
    "\n",
    "    return train,validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Retrieve indexes from the split*\n",
    "\n",
    "What we want to do is split the data into half training and half validation so instead of doing it directly on the samples of the dataset we simply can yield a division on indexes (numbers). Doing this we get two sets of numbers that will be the samples to retrieve from the dataset.\n",
    "\n",
    "Daset -> 392 samples\n",
    "\n",
    "training set -> 196 samples\n",
    "\n",
    "validation set -> 196 samples\n",
    "\n",
    "I.e \n",
    "if in the first set (the training), we have number 67 then the sample 67 will be part of the training set and the same is for validation.\n",
    "This ease the way of splitting the set cause we simply do a set division on numbers NOT DIRECTLY ON DATA SAMPLES.\n",
    "\n",
    "written by Lamberto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this are the indexes of the training set : [327, 57, 12, 379, 140, 125, 114, 71, 377, 52, 346, 388, 279, 44, 302, 216, 16, 15, 47, 111, 119, 258, 308, 13, 287, 101, 332, 359, 380, 214, 112, 229, 301, 142, 3, 81, 376, 174, 358, 79, 110, 172, 382, 373, 194, 49, 183, 176, 309, 135, 22, 235, 274, 63, 193, 40, 282, 150, 321, 316, 185, 295, 98, 35, 23, 116, 148, 336, 371, 51, 347, 353, 232, 186, 83, 189, 181, 107, 136, 36, 311, 87, 273, 386, 317, 236, 333, 138, 285, 361, 166, 28, 117, 375, 161, 205, 137, 33, 108, 290, 297, 293, 255, 202, 234, 73, 342, 384, 126, 275, 134, 219, 204, 331, 383, 70, 260, 252, 46, 24, 56, 78, 356, 355, 32, 197, 195, 239, 128, 5, 58, 313, 354, 390, 334, 222, 80, 326, 0, 244, 184, 224, 251, 67, 263, 265, 45, 129, 233, 27, 256, 160, 76, 215, 163, 339, 155, 50, 39, 95, 246, 41, 304, 199, 303, 253, 153, 82, 369, 4, 300, 92, 212, 206, 270, 61, 14, 268, 145, 20, 21, 187, 124, 208, 17, 305, 335, 196, 267, 203, 168, 121, 387, 42, 248, 227]\n",
      "this are the indexes of the test set : [1, 2, 6, 7, 8, 9, 10, 11, 18, 19, 25, 26, 29, 30, 31, 34, 37, 38, 43, 48, 53, 54, 55, 59, 60, 62, 64, 65, 66, 68, 69, 72, 74, 75, 77, 84, 85, 86, 88, 89, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 104, 105, 106, 109, 113, 115, 118, 120, 122, 123, 127, 130, 131, 132, 133, 139, 141, 143, 144, 146, 147, 149, 151, 152, 154, 156, 157, 158, 159, 162, 164, 165, 167, 169, 170, 171, 173, 175, 177, 178, 179, 180, 182, 188, 190, 191, 192, 198, 200, 201, 207, 209, 210, 211, 213, 217, 218, 220, 221, 223, 225, 226, 228, 230, 231, 237, 238, 240, 241, 242, 243, 245, 247, 249, 250, 254, 257, 259, 261, 262, 264, 266, 269, 271, 272, 276, 277, 278, 280, 281, 283, 284, 286, 288, 289, 291, 292, 294, 296, 298, 299, 306, 307, 310, 312, 314, 315, 318, 319, 320, 322, 323, 324, 325, 328, 329, 330, 337, 338, 340, 341, 343, 344, 345, 348, 349, 350, 351, 352, 357, 360, 362, 363, 364, 365, 366, 367, 368, 370, 372, 374, 378, 381, 385, 389, 391]\n"
     ]
    }
   ],
   "source": [
    "# retrieve the splitted sets\n",
    "train, test = randomize_split(392, 392//2)\n",
    "print(\"this are the indexes of the training set :\", train)\n",
    "print(\"this are the indexes of the test set :\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is : \n",
      "       mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "327  44.6          4          91.0          67    1850          13.8    80   \n",
      "\n",
      "     origin                 name  \n",
      "327       3  honda civic 1500 gl   \n",
      "\n",
      "linear variable with intercept :\n",
      "      const  horsepower\n",
      "327    1.0          67\n",
      "57     1.0          80\n",
      "12     1.0         150\n",
      "379    1.0          67\n",
      "140    1.0          67\n",
      "..     ...         ...\n",
      "121    1.0         110\n",
      "387    1.0          86\n",
      "42     1.0         170\n",
      "248    1.0         140\n",
      "227    1.0         180\n",
      "\n",
      "[196 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now we have only the indexes so what we need to do is actually split the dataset with the real features\n",
    "\n",
    "# first the training set\n",
    "\n",
    "# The .iloc indexer is used for integer-location based indexing in pandas. \n",
    "# It allows you to select rows and columns by their integer position (i.e., index numbers).\n",
    "# So the iloc is exactly what we need to map our features to our indexes\n",
    "# the syntax is iloc[row_indices, column_indices]\n",
    "# in our case we want the train indexes as rows and all the columns\n",
    "train = Auto.iloc[train, :]\n",
    "print(\"train set is : \\n\",train.head(1),\"\\n\")\n",
    "\n",
    "# Now we retrieved the features from the dataset and we are inetreseted in specific columns\n",
    "# the specific columns are horsepower and mpg\n",
    "# mpg is what we want to predict so the dependent variable\n",
    "# horsepower is what we use to predict so the indipendent variable\n",
    "x_train = train['horsepower']\n",
    "y_train = train['mpg']\n",
    "# remember the intercept is to add \n",
    "x_train = sm.add_constant(x_train)\n",
    "print(\"linear variable with intercept :\\n\",(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the test set : \n",
      "     mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "1  15.0          8         350.0         165    3693          11.5    70   \n",
      "\n",
      "   origin               name  \n",
      "1       1  buick skylark 320   \n",
      "\n",
      "this is the testing variable with intercept : \n",
      "      const  horsepower\n",
      "1      1.0         165\n",
      "2      1.0         150\n",
      "6      1.0         220\n",
      "7      1.0         215\n",
      "8      1.0         225\n",
      "..     ...         ...\n",
      "378    1.0          67\n",
      "381    1.0          85\n",
      "385    1.0          84\n",
      "389    1.0          84\n",
      "391    1.0          82\n",
      "\n",
      "[196 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# now the validation set\n",
    "\n",
    "testing = Auto.iloc[test,:]\n",
    "print(\"this is the test set : \\n\", testing.head(1), \"\\n\")\n",
    "\n",
    "y_test = testing['mpg']\n",
    "x_test = testing['horsepower']\n",
    "x_test = sm.add_constant(x_test)\n",
    "print(\"this is the testing variable with intercept : \\n\", x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression on the training set: `mpg`$= \\beta_0 + \\beta_1$ `horsepower` $+\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   286.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Oct 2024</td> <th>  Prob (F-statistic):</th> <td>4.47e-40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:30:09</td>     <th>  Log-Likelihood:    </th> <td> -573.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   196</td>      <th>  AIC:               </th> <td>   1151.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   194</td>      <th>  BIC:               </th> <td>   1158.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>   39.7094</td> <td>    1.011</td> <td>   39.271</td> <td> 0.000</td> <td>   37.715</td> <td>   41.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th> <td>   -0.1620</td> <td>    0.010</td> <td>  -16.930</td> <td> 0.000</td> <td>   -0.181</td> <td>   -0.143</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.055</td> <th>  Durbin-Watson:     </th> <td>   1.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.029</td> <th>  Jarque-Bera (JB):  </th> <td>   6.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.420</td> <th>  Prob(JB):          </th> <td>  0.0341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.350</td> <th>  Cond. No.          </th> <td>    330.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.596   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.594   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     286.6   \\\\\n",
       "\\textbf{Date:}             & Wed, 02 Oct 2024 & \\textbf{  Prob (F-statistic):} &  4.47e-40   \\\\\n",
       "\\textbf{Time:}             &     08:30:09     & \\textbf{  Log-Likelihood:    } &   -573.57   \\\\\n",
       "\\textbf{No. Observations:} &         196      & \\textbf{  AIC:               } &     1151.   \\\\\n",
       "\\textbf{Df Residuals:}     &         194      & \\textbf{  BIC:               } &     1158.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}      &      39.7094  &        1.011     &    39.271  &         0.000        &       37.715    &       41.704     \\\\\n",
       "\\textbf{horsepower} &      -0.1620  &        0.010     &   -16.930  &         0.000        &       -0.181    &       -0.143     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  7.055 & \\textbf{  Durbin-Watson:     } &    1.887  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.029 & \\textbf{  Jarque-Bera (JB):  } &    6.755  \\\\\n",
       "\\textbf{Skew:}          &  0.420 & \\textbf{  Prob(JB):          } &   0.0341  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.350 & \\textbf{  Cond. No.          } &     330.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.596\n",
       "Model:                            OLS   Adj. R-squared:                  0.594\n",
       "Method:                 Least Squares   F-statistic:                     286.6\n",
       "Date:                Wed, 02 Oct 2024   Prob (F-statistic):           4.47e-40\n",
       "Time:                        08:30:09   Log-Likelihood:                -573.57\n",
       "No. Observations:                 196   AIC:                             1151.\n",
       "Df Residuals:                     194   BIC:                             1158.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         39.7094      1.011     39.271      0.000      37.715      41.704\n",
       "horsepower    -0.1620      0.010    -16.930      0.000      -0.181      -0.143\n",
       "==============================================================================\n",
       "Omnibus:                        7.055   Durbin-Watson:                   1.887\n",
       "Prob(Omnibus):                  0.029   Jarque-Bera (JB):                6.755\n",
       "Skew:                           0.420   Prob(JB):                       0.0341\n",
       "Kurtosis:                       3.350   Cond. No.                         330.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we wanna fit a linear regression predicting mpg regressed on horsepower\n",
    "# we are now working not with the whole set but with the training set\n",
    "# we use OLS and not ols because we are handling our own sets and the OLS gives the possibiity to \n",
    "# pass directly our created design matrix (intercept + variables), that we created before\n",
    "lm1 = sm.OLS(y_train,x_train)\n",
    "# fit the model -> actually do the linear regression\n",
    "lm1 = lm1.fit()\n",
    "# summary gives us the result of the linear regression\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSE on the 196 observations that are the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      12.975070\n",
       "2      15.405466\n",
       "6       4.063616\n",
       "7       4.873748\n",
       "8       3.253484\n",
       "         ...    \n",
       "378    28.853660\n",
       "381    25.937184\n",
       "385    26.099211\n",
       "389    26.099211\n",
       "391    26.423264\n",
       "Length: 196, dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to compute the MSE on the validation we want to use the model to predict the validation y's by fitting the model with\n",
    "# the x's that it never saw which are the x's of the validation set \n",
    "predictions = lm1.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>reality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.975070</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.405466</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.063616</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.873748</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.253484</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>28.853660</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>25.937184</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>26.099211</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>26.099211</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>26.423264</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  reality\n",
       "1    12.975070     15.0\n",
       "2    15.405466     18.0\n",
       "6     4.063616     14.0\n",
       "7     4.873748     14.0\n",
       "8     3.253484     14.0\n",
       "..         ...      ...\n",
       "378  28.853660     32.0\n",
       "381  25.937184     38.0\n",
       "385  26.099211     36.0\n",
       "389  26.099211     32.0\n",
       "391  26.423264     31.0\n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we to compute MSE we are basically gonna compare them with the actual real y's that are in our y_test\n",
    "# first we can have a visual comparison \n",
    "comparison = pd.DataFrame({\n",
    "    \"predicted\" : predictions,\n",
    "    \"reality\" : y_test \n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_se</th>\n",
       "      <th>mean_ci_lower</th>\n",
       "      <th>mean_ci_upper</th>\n",
       "      <th>obs_ci_lower</th>\n",
       "      <th>obs_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>28.853660</td>\n",
       "      <td>0.453091</td>\n",
       "      <td>27.960043</td>\n",
       "      <td>29.747277</td>\n",
       "      <td>19.858474</td>\n",
       "      <td>37.848846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>26.747316</td>\n",
       "      <td>0.376829</td>\n",
       "      <td>26.004108</td>\n",
       "      <td>27.490524</td>\n",
       "      <td>17.765826</td>\n",
       "      <td>35.728807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.405466</td>\n",
       "      <td>0.577381</td>\n",
       "      <td>14.266716</td>\n",
       "      <td>16.544216</td>\n",
       "      <td>6.382630</td>\n",
       "      <td>24.428302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>28.853660</td>\n",
       "      <td>0.453091</td>\n",
       "      <td>27.960043</td>\n",
       "      <td>29.747277</td>\n",
       "      <td>19.858474</td>\n",
       "      <td>37.848846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>28.853660</td>\n",
       "      <td>0.453091</td>\n",
       "      <td>27.960043</td>\n",
       "      <td>29.747277</td>\n",
       "      <td>19.858474</td>\n",
       "      <td>37.848846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>21.886523</td>\n",
       "      <td>0.337789</td>\n",
       "      <td>21.220314</td>\n",
       "      <td>22.552733</td>\n",
       "      <td>12.911076</td>\n",
       "      <td>30.861971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>25.775158</td>\n",
       "      <td>0.351042</td>\n",
       "      <td>25.082808</td>\n",
       "      <td>26.467507</td>\n",
       "      <td>16.797733</td>\n",
       "      <td>34.752583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12.164937</td>\n",
       "      <td>0.743584</td>\n",
       "      <td>10.698391</td>\n",
       "      <td>13.631484</td>\n",
       "      <td>3.094900</td>\n",
       "      <td>21.234975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>17.025730</td>\n",
       "      <td>0.501072</td>\n",
       "      <td>16.037482</td>\n",
       "      <td>18.013979</td>\n",
       "      <td>8.020651</td>\n",
       "      <td>26.030810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>10.544673</td>\n",
       "      <td>0.830765</td>\n",
       "      <td>8.906183</td>\n",
       "      <td>12.183163</td>\n",
       "      <td>1.445252</td>\n",
       "      <td>19.644094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n",
       "327  28.853660  0.453091      27.960043      29.747277     19.858474   \n",
       "57   26.747316  0.376829      26.004108      27.490524     17.765826   \n",
       "12   15.405466  0.577381      14.266716      16.544216      6.382630   \n",
       "379  28.853660  0.453091      27.960043      29.747277     19.858474   \n",
       "140  28.853660  0.453091      27.960043      29.747277     19.858474   \n",
       "..         ...       ...            ...            ...           ...   \n",
       "121  21.886523  0.337789      21.220314      22.552733     12.911076   \n",
       "387  25.775158  0.351042      25.082808      26.467507     16.797733   \n",
       "42   12.164937  0.743584      10.698391      13.631484      3.094900   \n",
       "248  17.025730  0.501072      16.037482      18.013979      8.020651   \n",
       "227  10.544673  0.830765       8.906183      12.183163      1.445252   \n",
       "\n",
       "     obs_ci_upper  \n",
       "327     37.848846  \n",
       "57      35.728807  \n",
       "12      24.428302  \n",
       "379     37.848846  \n",
       "140     37.848846  \n",
       "..            ...  \n",
       "121     30.861971  \n",
       "387     34.752583  \n",
       "42      21.234975  \n",
       "248     26.030810  \n",
       "227     19.644094  \n",
       "\n",
       "[196 rows x 6 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the predict() function gives us simply the result of the linear model and nothing more\n",
    "# while when we use get_prediction() we get a more structured output with many more informations\n",
    "prediction_stats =  lm1.get_prediction(x_train)\n",
    "summary = prediction_stats.summary_frame()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(111.47647729910564)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now based on our prediction stats lets retrieve the actual values and compute the Mean Squared Error\n",
    "# values are stored under the field : mean\n",
    "predicted_values = summary[\"mean\"]\n",
    "# the followinig will compute the mse between real values and predicted values of the test data\n",
    "mse = sm.tools.eval_measures.mse(y_test,predicted_values)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now estimate also the MSE for the quadradic and cubic regressions.\n",
    "\n",
    "The ISLwR book found an estimated test MSE of (23.27, 18.27, and 18.79) for the three regression models. \n",
    "\n",
    "Set another seed and create a different split of your data - do you get different results? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now given that we are tryin to find the mse for different mse's for different models, \n",
    "# we can simply create a function where we will pass our model so that it gives back the mse\n",
    "\n",
    "def compute_mse(model,x,y):\n",
    "    # simply compute the same procedure we've done for the one above\n",
    "    pred_stats = model.get_prediction(x)\n",
    "    summary_frame = pred_stats.summary_frame()\n",
    "    pred_values = summary_frame['mean']\n",
    "    calculated_mse = sm.tools.eval_measures.mse(y,pred_values)  \n",
    "    return calculated_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HOW DO WE FIND THE QUADRATIC AND CUBIC REGRESSIONS??*\n",
    "\n",
    "to find what we are searching for we can use some built-in functions from sklearn. What we are gonna use is PolynomialFeatures : \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "written by Lamberto Ragnolini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the new matrix is : \n",
      " (196, 3) \n",
      "\n",
      "while the new training matrix is : \n",
      " [[1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 1.4500e+02 2.1025e+04]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 6.5000e+01 4.2250e+03]\n",
      " [1.0000e+00 6.3000e+01 3.9690e+03]\n",
      " [1.0000e+00 5.2000e+01 2.7040e+03]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 6.9000e+01 4.7610e+03]\n",
      " [1.0000e+00 5.8000e+01 3.3640e+03]\n",
      " [1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 1.1200e+02 1.2544e+04]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 6.0000e+01 3.6000e+03]\n",
      " [1.0000e+00 2.2500e+02 5.0625e+04]\n",
      " [1.0000e+00 1.5500e+02 2.4025e+04]\n",
      " [1.0000e+00 4.6000e+01 2.1160e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 1.0500e+02 1.1025e+04]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 6.8000e+01 4.6240e+03]\n",
      " [1.0000e+00 1.0700e+02 1.1449e+04]\n",
      " [1.0000e+00 1.9000e+02 3.6100e+04]\n",
      " [1.0000e+00 6.5000e+01 4.2250e+03]\n",
      " [1.0000e+00 5.2000e+01 2.7040e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 8.6000e+01 7.3960e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 9.2000e+01 8.4640e+03]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 6.0000e+01 3.6000e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 7.9000e+01 6.2410e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 6.3000e+01 3.9690e+03]\n",
      " [1.0000e+00 1.1500e+02 1.3225e+04]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 5.2000e+01 2.7040e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 1.0500e+02 1.1025e+04]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 7.7000e+01 5.9290e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 1.1300e+02 1.2769e+04]\n",
      " [1.0000e+00 4.9000e+01 2.4010e+03]\n",
      " [1.0000e+00 9.3000e+01 8.6490e+03]\n",
      " [1.0000e+00 9.2000e+01 8.4640e+03]\n",
      " [1.0000e+00 6.8000e+01 4.6240e+03]\n",
      " [1.0000e+00 7.6000e+01 5.7760e+03]\n",
      " [1.0000e+00 6.5000e+01 4.2250e+03]\n",
      " [1.0000e+00 7.4000e+01 5.4760e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 8.1000e+01 6.5610e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 1.3700e+02 1.8769e+04]\n",
      " [1.0000e+00 1.2500e+02 1.5625e+04]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 8.3000e+01 6.8890e+03]\n",
      " [1.0000e+00 7.2000e+01 5.1840e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 1.3800e+02 1.9044e+04]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 8.3000e+01 6.8890e+03]\n",
      " [1.0000e+00 1.9300e+02 3.7249e+04]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 1.0200e+02 1.0404e+04]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 1.0500e+02 1.1025e+04]\n",
      " [1.0000e+00 7.2000e+01 5.1840e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 7.1000e+01 5.0410e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 8.9000e+01 7.9210e+03]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 9.6000e+01 9.2160e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 1.3300e+02 1.7689e+04]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 1.4500e+02 2.1025e+04]\n",
      " [1.0000e+00 7.2000e+01 5.1840e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 1.1200e+02 1.2544e+04]\n",
      " [1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 1.4500e+02 2.1025e+04]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 6.9000e+01 4.7610e+03]\n",
      " [1.0000e+00 1.1600e+02 1.3456e+04]\n",
      " [1.0000e+00 7.6000e+01 5.7760e+03]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 1.0000e+02 1.0000e+04]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 1.9800e+02 3.9204e+04]\n",
      " [1.0000e+00 5.4000e+01 2.9160e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 7.9000e+01 6.2410e+03]\n",
      " [1.0000e+00 8.4000e+01 7.0560e+03]\n",
      " [1.0000e+00 1.3000e+02 1.6900e+04]\n",
      " [1.0000e+00 9.2000e+01 8.4640e+03]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]\n",
      " [1.0000e+00 1.3000e+02 1.6900e+04]\n",
      " [1.0000e+00 5.2000e+01 2.7040e+03]\n",
      " [1.0000e+00 8.3000e+01 6.8890e+03]\n",
      " [1.0000e+00 1.0500e+02 1.1025e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 1.5500e+02 2.4025e+04]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 7.2000e+01 5.1840e+03]\n",
      " [1.0000e+00 6.5000e+01 4.2250e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 2.1000e+02 4.4100e+04]\n",
      " [1.0000e+00 1.0500e+02 1.1025e+04]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 7.6000e+01 5.7760e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 5.8000e+01 3.3640e+03]\n",
      " [1.0000e+00 1.4500e+02 2.1025e+04]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 1.5300e+02 2.3409e+04]\n",
      " [1.0000e+00 1.7500e+02 3.0625e+04]\n",
      " [1.0000e+00 6.0000e+01 3.6000e+03]\n",
      " [1.0000e+00 1.8000e+02 3.2400e+04]\n",
      " [1.0000e+00 1.1500e+02 1.3225e+04]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 8.8000e+01 7.7440e+03]\n",
      " [1.0000e+00 7.2000e+01 5.1840e+03]\n",
      " [1.0000e+00 8.0000e+01 6.4000e+03]\n",
      " [1.0000e+00 7.4000e+01 5.4760e+03]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 7.0000e+01 4.9000e+03]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 1.3000e+02 1.6900e+04]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 1.6500e+02 2.7225e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 8.7000e+01 7.5690e+03]\n",
      " [1.0000e+00 9.0000e+01 8.1000e+03]\n",
      " [1.0000e+00 1.2000e+02 1.4400e+04]\n",
      " [1.0000e+00 9.5000e+01 9.0250e+03]\n",
      " [1.0000e+00 1.0800e+02 1.1664e+04]\n",
      " [1.0000e+00 8.5000e+01 7.2250e+03]\n",
      " [1.0000e+00 1.1500e+02 1.3225e+04]\n",
      " [1.0000e+00 8.4000e+01 7.0560e+03]\n",
      " [1.0000e+00 5.3000e+01 2.8090e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 7.8000e+01 6.0840e+03]\n",
      " [1.0000e+00 1.1000e+02 1.2100e+04]\n",
      " [1.0000e+00 8.6000e+01 7.3960e+03]\n",
      " [1.0000e+00 1.7000e+02 2.8900e+04]\n",
      " [1.0000e+00 1.4000e+02 1.9600e+04]\n",
      " [1.0000e+00 1.8000e+02 3.2400e+04]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first thing first retrieve the desidered column feature we will use\n",
    "# we transform it into a numpy array\n",
    "x = train[\"horsepower\"].to_numpy()\n",
    "# thanks to the built in function we can retrieve the quadratic polynomial,\n",
    "# here we simply create an object that will be able to give us back the new features later\n",
    "new_quadratic_features = PolynomialFeatures(degree=2)\n",
    "# from teacher :\n",
    "#   if you don't reshape X as such if it's a single feature\n",
    "#   vector you will get an error telling you to do so\n",
    "# by reshaping we change the dimension from (n) to (n,1), it is like a matrix \n",
    "# the fit_transform is able to transform the features and give us back also the new quadratic ones\n",
    "x_quadratic_train = new_quadratic_features.fit_transform(x.reshape(-1,1))\n",
    "print(\"the shape of the new matrix is : \\n\",x_quadratic_train.shape, \"\\n\" )\n",
    "print(\"while the new training matrix is : \\n\", x_quadratic_train,\"\\n\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quadratic mse is : 21.202064720220054\n"
     ]
    }
   ],
   "source": [
    "# Now it is time to fit the model \n",
    "# do exactly what we did before but with the new features\n",
    "lmq = sm.OLS(y_train,x_quadratic_train).fit()\n",
    "lmq.summary() # print this to see the parameters of the regression\n",
    "\n",
    "# now get the mse of the regression\n",
    "# remembeeer that we are working with 2d's array so we have to transform\n",
    "x_quadratic_test = new_quadratic_features.fit_transform((testing['horsepower']).to_numpy().reshape(-1,1))\n",
    "quadratic_mse = compute_mse(lmq, x_quadratic_test,y_test)\n",
    "print(\"the quadratic mse is :\", quadratic_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOW DO THE SAME THING FOR THE CUBIC MODEL*\n",
    "\n",
    "follow the same procedure that was done for the quadratic but change the polynomial degree to cubic meaning degree=3\n",
    "\n",
    "written by Lamberto Ragnolini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cubic mse is :  21.213560453128846\n"
     ]
    }
   ],
   "source": [
    "# cubic regression\n",
    "new_cubic_features = PolynomialFeatures(degree=3)\n",
    "\n",
    "# create new fetaures\n",
    "new_x = train['horsepower']\n",
    "x_cubic_train = new_cubic_features.fit_transform(new_x.to_numpy().reshape(-1,1))\n",
    "x_cubic_testing = new_cubic_features.fit_transform(testing['horsepower'].to_numpy().reshape(-1,1))\n",
    "\n",
    "# do the regression\n",
    "lmc = sm.OLS(y_train,x_cubic_train).fit()\n",
    "lmc.summary() #print this if you wanna see the regression parameters\n",
    "\n",
    "# get the mse of the cubic regression\n",
    "cubic_mse = compute_mse(lmc,x_cubic_testing, y_test)\n",
    "print(\"the cubic mse is : \", cubic_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model would you choose based on the estimated MSE? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would personally choose the quadratic one because it's mse is the lowest\n",
    "\n",
    "written by Lamberto Ragnolini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you instead train the models on the entire data and perform model selection via F-tests or AIC, would you choose differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first model aic is :  2361.3236578374017\n",
      "qudratic model aic is :  2272.353522359692\n",
      "cubic model aic is :  2273.531296711554\n"
     ]
    }
   ],
   "source": [
    "# The only way to find out is to actually compute them so: \n",
    "\n",
    "lm1_total_data = ols(\"mpg ~ horsepower\", Auto).fit()\n",
    "lm_quadratic_total_data = ols(\"mpg ~ horsepower + I(horsepower**2)\",Auto).fit()\n",
    "lm_cubic_total_data = ols(\"mpg ~ horsepower +I(horsepower**2) + I(horsepower**3)\", Auto).fit()\n",
    "\n",
    "print(\"first model aic is : \", lm1_total_data.aic)\n",
    "print(\"qudratic model aic is : \", lm_quadratic_total_data.aic)\n",
    "print(\"cubic model aic is : \", lm_cubic_total_data.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial choice that someone would make, the preferred model to choose is the second one because it has the lower aic. As the teacher points out though, aic tends to favorite easier models and sometimes this results in underfitting so we gotta be careful when we actually make our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross-validation\n",
    "\n",
    "Implement leave-one-out cross-validation and use it to estimate the MSE for the three regression models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WATCH NOTES if you don't remember how this works\n",
    "# general concept :\n",
    "#   split data into n partitions, from every partition leave one point out that is gonna be used for testing\n",
    "#   given this approach we will have ith test mse for each partition\n",
    "#   this leads to a test mse = AVG(test error) of all the mse'set\n",
    "\n",
    "# As ususal we don't have to complicate our life by implementing from scratch the methods but we can use built-in functions\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# declare and instance of the leaveOneOut method\n",
    "loocv = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the desidered model\n",
    "# more explanation below in the call\n",
    "\n",
    "def compute_desidered_formula_model(i):\n",
    "    \n",
    "    formula = \"mpg ~ horsepower\"\n",
    "\n",
    "    if i == 1 :\n",
    "        return formula\n",
    "    else :\n",
    "        for j in range(2,i+1):\n",
    "            formula += f\" + I(horsepower**{j})\"\n",
    "\n",
    "    return formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.231513517929244\n",
      "19.248213124489528\n",
      "19.334984064022766\n"
     ]
    }
   ],
   "source": [
    "# We want now to estimate the mse using leaveOneOut approach of the three created models\n",
    "\n",
    "# create a variable where to store the mse we get as answer\n",
    "mses = [0] *3\n",
    "\n",
    "# the splitting basically splits the data into training and testing data for each iteration, that are gonna be stored in the two train,test data,\n",
    "# enumerate basically gives the counter increment to i\n",
    "# To recap, this iteration iterates for n times with n being the number of data points in the dataset, every time it creates a :\n",
    "#   training set [0,...,n-1] containing n-1 elements\n",
    "#   test set [i] containing the ith element of the iteration \n",
    "for i,(train,test) in enumerate(loocv.split(Auto)):\n",
    "    # now we have to keep track of the models we wanna create\n",
    "    model = []\n",
    "    # remember the indexes fact, now we have to retrieve the real feautures\n",
    "    train_loocv = Auto.iloc[train] #training features\n",
    "    test_loocv = Auto.iloc[test] #test feature\n",
    "    real_y = test_loocv['mpg'] # dependent variable we are interested in, the real one\n",
    "    test_x = test_loocv['horsepower'] #independent variable we are interested in\n",
    "\n",
    "    # now we wanna test the models we created before with the quadratic, cubic etc...\n",
    "    # to do this we can create an auxilliary function that gives us back the model desidered\n",
    "    for j in range(1,4):\n",
    "        # retrieve formula \n",
    "        formula = compute_desidered_formula_model(j)\n",
    "        # compute regression\n",
    "        lm = ols(formula,train_loocv).fit()\n",
    "        # save the computed regression\n",
    "        model.append(lm)\n",
    "    \n",
    "    for l in range(len(model)):\n",
    "        # As before calculate the mse for the current model\n",
    "        pred = model[l].get_prediction(test_x).summary_frame()[\"mean\"]\n",
    "        # BEE CAREFULL here WE ARE ADDING the msees for every iteration, THIS IS VERY IMPORTANT\n",
    "        # the overall mse is the avg(test error) and to get it we need to have the overall result of all the calculated mses\n",
    "        mses[l] += sm.tools.eval_measures.mse(real_y,pred)\n",
    "    \n",
    "for i in range(len(mses)):\n",
    "    avg_mse = mses[i] / (Auto.shape[0])\n",
    "    print(avg_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation\n",
    "\n",
    "Estimate the MSE now from k-fold cross-validation that you implement yourself. If you set k=10, then you get Figure 5.6 from the book, but you are welcome to choose another k. Make sure that you properly randomise observations into the k folds! One way is to first make a random permutation of row indices and then chop the re-ordered dataset into k (roughly) even parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate of the MSE can also be computed automatially with sklearn. In fact, sklearn provides a lot of functionality for cross-validation. However, our objective right now is to understand how the algorithms work, so that you learn to reflect on them and use the automatic functionality appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation leaves 1 kth of the data out at a time\n",
    "# in the ith held-out we compute the MSE and then we take the avg of all the calculated mse\n",
    "# to work with k fold just import it from sklearn\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "k = 10\n",
    "k_folds = KFold(n_splits = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.439933652339864\n",
      "21.235840055806367\n",
      "21.3366061836332\n"
     ]
    }
   ],
   "source": [
    "models_mses = [0] * 3\n",
    "\n",
    "# we have to properly randomize the observations in the folds so we need to take\n",
    "# into consideration the permutation suggested in the markdown above\n",
    "\n",
    "# the sample function randomly samples lines from the dataset \n",
    "# frac=1 means that we are sampling 100% of the row, so effectively shuffling the entire dataframe\n",
    "# after the indexing the indexes are messed up so we use reset_index to reindex the new shuffled column 0 to n-1\n",
    "# drop = true means that we are dropping the old indexes\n",
    "auto_sh = Auto.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# do the exact same thing we did above\n",
    "\n",
    "for i,(train,test) in enumerate(k_folds.split(auto_sh)):\n",
    "    # now we have to keep track of the models we wanna create\n",
    "    model = []\n",
    "    # remember the indexes fact, now we have to retrieve the real feautures\n",
    "    train_loocv = Auto.iloc[train] #training features\n",
    "    test_loocv = Auto.iloc[test] #test feature\n",
    "    real_y = test_loocv['mpg'] # dependent variable we are interested in, the real one\n",
    "    test_x = test_loocv['horsepower'] #independent variable we are interested in\n",
    "\n",
    "    # now we wanna test the models we created before with the quadratic, cubic etc...\n",
    "    # to do this we can create an auxilliary function that gives us back the model desidered\n",
    "    for j in range(1,4):\n",
    "        # retrieve formula \n",
    "        formula = compute_desidered_formula_model(j)\n",
    "        # compute regression\n",
    "        lm = ols(formula,train_loocv).fit()\n",
    "        # save the computed regression\n",
    "        model.append(lm)\n",
    "    \n",
    "    for l in range(len(model)):\n",
    "        # As before calculate the mse for the current model\n",
    "        pred = model[l].get_prediction(test_x).summary_frame()[\"mean\"]\n",
    "        # BEE CAREFULL here WE ARE ADDING the msees for every iteration, THIS IS VERY IMPORTANT\n",
    "        # the overall mse is the avg(test error) and to get it we need to have the overall result of all the calculated mses\n",
    "        models_mses[l] += sm.tools.eval_measures.mse(real_y,pred)\n",
    "    \n",
    "for i in range(len(models_mses)):\n",
    "    avg_k_mse = models_mses[i] / k\n",
    "    print(avg_k_mse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your code with a for-loop that iterates over increasing orders of the polynomial (1 to 10). Note that is possible to autogenerate a design matrix with all the terms of the polynomial with functions such as `sklearn.preprocessing.PolynomialFeatures` (It should also work for several features; a degree 2 polynomial in features x1 and x2 would need  $1+x_1+x_2+x_1x_2+x_1^2+x_2^2$, i.e. also the term $x_1x_2$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for degree 1  : 27.439933652339864\n",
      "mse for degree 2  : 21.235840055806367\n",
      "mse for degree 3  : 21.3366061836332\n",
      "mse for degree 4  : 21.35388692299275\n",
      "mse for degree 5  : 20.905668490831523\n",
      "mse for degree 6  : 20.8970104598246\n",
      "mse for degree 7  : 20.95533584531355\n",
      "mse for degree 8  : 25.655499601754162\n",
      "mse for degree 9  : 25.912028574581917\n",
      "mse for degree 10  : 65.60749032372748\n"
     ]
    }
   ],
   "source": [
    "models_mses = [0] * 10\n",
    "\n",
    "# do the exact same thing we did above\n",
    "\n",
    "for i,(train,test) in enumerate(k_folds.split(auto_sh)):\n",
    "    model = []\n",
    "    train_loocv = Auto.iloc[train] #training features\n",
    "    test_loocv = Auto.iloc[test] #test feature\n",
    "    real_y = test_loocv['mpg'] # dependent variable we are interested in, the real one\n",
    "    test_x = test_loocv['horsepower'] #independent variable we are interested in\n",
    "\n",
    "    # Now instead of stopping only to the cubic we fit our model with polynomials of up to 10th degree\n",
    "    for j in range(1,11):\n",
    "        formula = compute_desidered_formula_model(j)\n",
    "        lm = ols(formula,train_loocv).fit()\n",
    "        model.append(lm)\n",
    "    \n",
    "    for l in range(len(model)):\n",
    "        pred = model[l].get_prediction(test_x).summary_frame()[\"mean\"]\n",
    "        models_mses[l] += sm.tools.eval_measures.mse(real_y,pred)\n",
    "    \n",
    "avg_mses = []\n",
    "for i in range(len(models_mses)):\n",
    "    avg_mse = models_mses[i] / 10\n",
    "    avg_mses.append(avg_mse)\n",
    "    print(f\"mse for degree {i+1}  : {avg_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting MSE curve against the degree of the polynomial (this is the complexity or the flexibility). \n",
    "\n",
    "Do you get the same conclusion as before regarding your choice of model?\n",
    "\n",
    "Was it faster than LOOCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7fac307ec400>,\n",
       "  <matplotlib.axis.XTick at 0x7fac307ecb50>,\n",
       "  <matplotlib.axis.XTick at 0x7fac307ecc40>,\n",
       "  <matplotlib.axis.XTick at 0x7fac3084a770>,\n",
       "  <matplotlib.axis.XTick at 0x7fac3084a8c0>,\n",
       "  <matplotlib.axis.XTick at 0x7fac3084b9d0>,\n",
       "  <matplotlib.axis.XTick at 0x7fac3084a200>,\n",
       "  <matplotlib.axis.XTick at 0x7fac3084ba90>,\n",
       "  <matplotlib.axis.XTick at 0x7fac30874820>,\n",
       "  <matplotlib.axis.XTick at 0x7fac30875000>],\n",
       " [Text(1, 0, '1'),\n",
       "  Text(2, 0, '2'),\n",
       "  Text(3, 0, '3'),\n",
       "  Text(4, 0, '4'),\n",
       "  Text(5, 0, '5'),\n",
       "  Text(6, 0, '6'),\n",
       "  Text(7, 0, '7'),\n",
       "  Text(8, 0, '8'),\n",
       "  Text(9, 0, '9'),\n",
       "  Text(10, 0, '10')])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list(range(1,11)) generates the values 1-10 that will be on the x axis\n",
    "plt.plot(list(range(1,11)),avg_mses)\n",
    "plt.title(\"avg MSE for different degress 0 to 10\")\n",
    "plt.xlabel(\"degrees\")\n",
    "plt.ylabel(\"avg mses\")\n",
    "# ensures that all the units on the x-axis are displayed\n",
    "plt.xticks(list(range(1,11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer taken from Teaching assistant : \n",
    "\n",
    "Based on the above plot, we would still choose the 2nd degree polynomial.\n",
    "We can also notice that the KFold with 10 splits executed a lot faster than the LOOCV (Which makes sense, because LOOCV is esentially KFold with K=n, where n is the number of samples in your dataset. These numbers of folds correspond with how many times you execute the loop, so the more folds you have the slower the calculation will be)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage/Regularization\n",
    "\n",
    "Now try the two shrinkage methods we saw on Thursday, ridge regression and lasso. To make things a bit more exciting, include in your models all variables available in the dataset, and perhaps a few polynomial terms for `horsepower` now that you know they are relevant. The `name` variable is evidently an identifyer and should not be used as a predictor in the model.\n",
    "\n",
    "The Lasso estimates for coefficients $\\beta$ in a linear regression model are found by minimising $RSS + \\lambda\\sum_{j=1}^p|\\beta_j|$ rather than RSS. \n",
    "\n",
    "In Ridge regression, we estimate $\\beta$ by minimising $RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$ rather than RSS. \n",
    "\n",
    "In both cases, we need to set the regularation parameter $\\lambda$. **The parameter $\\lambda$ is called `alpha` in the Python implementations**\n",
    "\n",
    "\n",
    "The relevant method in the statsmodel library is `fit_regularized`, which you use in place of `fit` after instantiating an ols model. If `L1_wt=1`, the fit is the lasso (L1-penalty). Ridge regression (L2-penalty) is obtained by setting `L1_wt=0`.  The reason we need to set parameter `L1_wt` is that the method actually implements the more general 'elastic net' penalty, which is a convex combination of the Lasso penalty and the ridge regression penalty with weight `L1_wt`. The fitting method returns a `RegressionResults` object, from which you can extract `params` and `fittedvalues`, and it has a `predict` method just like the usual linear regression output from ols.\n",
    "\n",
    "The sklearn libary has methods `Ridge` and `Lasso`. \n",
    "\n",
    "**Set aside a test set of about 25% of the data that you can use at the end to estimate the MSE of your final model. Use the remaining data to train the regularised regression models.**\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "First, carry out a ridge regression where the regulation parameter $\\lambda$ is fixed at 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# we wanna keep 25% of the data for the testing part so 75% will be for the training\n",
    "\n",
    "# .shape[0] returns the number of rows, so we get the total number of samples\n",
    "train_indexes = int(Auto.shape[0] *0.75)\n",
    "# select from 0 to 294 (train_indexes)\n",
    "# the 1:8 means to select feautures from the second column till the last, have a look at the dataset to see which features toi leave out i.e name\n",
    "train_x = (Auto.iloc[:train_indexes,1:8]).to_numpy()\n",
    "# this selects only the values from the 0th column which are the true values 0 or 1\n",
    "train_y = (Auto.iloc[:train_indexes, 0]).to_numpy()\n",
    "# same for this two but for the testing data\n",
    "test_x = (Auto.iloc[train_indexes:, 1:8]).to_numpy()\n",
    "test_y = (Auto.iloc[train_indexes:, 0]).to_numpy()\n",
    "\n",
    "print(train_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.86250991322583\n"
     ]
    }
   ],
   "source": [
    "# now do ridge\n",
    "# start with alpha = 10\n",
    "# fit model\n",
    "ridge_regression = Ridge(alpha=10)\n",
    "ridge_regression.fit(train_x,train_y)\n",
    "\n",
    "# test the model always using the predict\n",
    "# calculate the mse for it\n",
    "predictions = ridge_regression.predict(test_x)\n",
    "ridge_mse = sm.tools.eval_measures.mse(test_y,predictions)\n",
    "print(ridge_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, $\\lambda = 10$ is a rather arbitrary suggestion. We need to decide on a suitable value for the hyperparameter $\\lambda$. This we can do using a dedicated validation set, or we could do it by cross-validation. We do the latter.\n",
    "\n",
    "Take a suitable range of $\\lambda \\in [0, \\infty)$, for instance you can take a sequence $10^i, i=-2, \\ldots, 5$. Using 5-fold cross-validation to estimate the MSE (call it `MSE_lambda`) for each $\\lambda$, make a plot of how MSE changes with $\\lambda$. Based on this plot, how would you choose $\\lambda$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE (val data) for ridge regression with lambda=-2: 12.81909362255712\n",
      "Avg MSE (val data) for ridge regression with lambda=-1: 12.818921174735584\n",
      "Avg MSE (val data) for ridge regression with lambda=0: 12.817316378911105\n",
      "Avg MSE (val data) for ridge regression with lambda=1: 12.810705977585448\n",
      "Avg MSE (val data) for ridge regression with lambda=2: 12.936294798447047\n",
      "Avg MSE (val data) for ridge regression with lambda=3: 13.637184809237393\n",
      "Avg MSE (val data) for ridge regression with lambda=4: 17.19582531048932\n",
      "Avg MSE (val data) for ridge regression with lambda=5: 19.33671937042007\n"
     ]
    }
   ],
   "source": [
    "# the following code is identical to the solution but yields problems so just copied code from solutions\n",
    "\n",
    "# retrieve the features\n",
    "# train_features = auto_sh.iloc[:train_indexes]\n",
    "\n",
    "# fold_five = KFold(n_splits=5)\n",
    "\n",
    "# model_mses = [0] * 8\n",
    "\n",
    "# for i,(train_i,test_i) in enumerate(fold_five.split(train_features)):\n",
    "#     model = []\n",
    "    \n",
    "#     # taken from solution\n",
    "#     train_loocv = train_features.iloc[train_i]\n",
    "#     y_train_loocv = train_loocv.iloc[:,0]\n",
    "#     train_loocv = train_loocv.iloc[:,1:8]\n",
    "#     val_loocv = train_features.iloc[test_i]\n",
    "#     true_vals = val_loocv.iloc[:, 0]\n",
    "#     val_loocv = val_loocv.iloc[:, 1:8]\n",
    "\n",
    "#     # now our lambda needs to be 10^j with j= -2...5\n",
    "#     for j in range(-2,6):\n",
    "#         lamda = 10**j\n",
    "#         ridge_reg = Ridge(alpha=lamda)\n",
    "#         ridge_reg.fit(train_loocv,y_train_loocv)\n",
    "#         model.append(ridge_reg)\n",
    "    \n",
    "#     for l in range(8):\n",
    "#         predictions = model[l].predict(val_loocv)\n",
    "#         model_mses[i] += sm.tools.eval_measures.mse(true_vals, predictions)\n",
    "    \n",
    "# avg_mses = []\n",
    "# for i in range(8):\n",
    "#     avg_mse = models_mses[i] / 5\n",
    "#     avg_mses.append(avg_mse)\n",
    "#     print(f\"mse for lamda {i-2}  : {avg_mse}\")\n",
    "\n",
    "train_data = auto_sh.iloc[:train_indexes]\n",
    "\n",
    "kf5 = KFold(n_splits=5)\n",
    "metrics = [0] * 8\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf5.split(train_data)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = train_data.iloc[train_idx]\n",
    "    y_train_loocv = train_loocv.iloc[:,0]\n",
    "    train_loocv = train_loocv.iloc[:,1:8]\n",
    "    val_loocv = train_data.iloc[val_idx]\n",
    "    true_vals = val_loocv.iloc[:, 0]\n",
    "    val_loocv = val_loocv.iloc[:, 1:8]\n",
    "    \n",
    "    for i in range(-2, 6):\n",
    "        \n",
    "        curr_lambda = 10**i\n",
    "        curr_ridge = Ridge(alpha = curr_lambda)\n",
    "        curr_ridge.fit(train_loocv, y_train_loocv)\n",
    "        models.append(curr_ridge)\n",
    "    \n",
    "    for i in range(8):\n",
    "\n",
    "        curr_preds =  models[i].predict(val_loocv)\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "mses = []\n",
    "for i in range(8):\n",
    "    avg_mse = metrics[i] / 5\n",
    "    mses.append(avg_mse)\n",
    "    print(f\"Avg MSE (val data) for ridge regression with lambda={i-2}: {avg_mse}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having chosen a $\\lambda$, would it be appropriate to report the corresponding `MSE_lambda` as the test error for the ridge regression?\n",
    "Compute the test MSE using the dedicated test set and compare to `MSE_lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.51363598342456\n"
     ]
    }
   ],
   "source": [
    "# the lowest mse is the one for lamda=1\n",
    "# so regress with that tuning parameter\n",
    "ridge_lowest_lamda = Ridge(alpha=1)\n",
    "ridge_lowest_lamda.fit(train_x,train_y)\n",
    "\n",
    "predictions = ridge_lowest_lamda.predict(test_x)\n",
    "mse = sm.tools.eval_measures.mse(test_y,predictions)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, once we have chosen the regularisation parameter $\\lambda$, we should preferably make use of *all of the data* to (re-)train the model, and then report the associated test MSE from the test data. Do this, and inspect the model estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Following the same procedure as for ridge regression, train a lasso regression where the regularisation parameter $\\lambda$ is selected by cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE (val data) for ridge regression with lambda=-2: 12.811170745105883\n",
      "Avg MSE (val data) for ridge regression with lambda=-1: 12.840102080858182\n",
      "Avg MSE (val data) for ridge regression with lambda=0: 13.272735294870765\n",
      "Avg MSE (val data) for ridge regression with lambda=1: 19.723326128330505\n",
      "Avg MSE (val data) for ridge regression with lambda=2: 20.308328470151285\n",
      "Avg MSE (val data) for ridge regression with lambda=3: 21.70849684721885\n",
      "Avg MSE (val data) for ridge regression with lambda=4: 62.44337977329243\n",
      "Avg MSE (val data) for ridge regression with lambda=5: 62.44337977329243\n"
     ]
    }
   ],
   "source": [
    "# we have to do THE EXACT SAME THING WE DID ABOVE BUT WHITH LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "train_data = auto_sh.iloc[:train_indexes]\n",
    "\n",
    "kf5 = KFold(n_splits=5)\n",
    "metrics = [0] * 8\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf5.split(train_data)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = train_data.iloc[train_idx]\n",
    "    y_train_loocv = train_loocv.iloc[:,0]\n",
    "    train_loocv = train_loocv.iloc[:,1:8]\n",
    "    val_loocv = train_data.iloc[val_idx]\n",
    "    true_vals = val_loocv.iloc[:, 0]\n",
    "    val_loocv = val_loocv.iloc[:, 1:8]\n",
    "    \n",
    "    for i in range(-2, 6):\n",
    "        \n",
    "        curr_lambda = 10**i\n",
    "        curr_ridge = Lasso(alpha = curr_lambda)\n",
    "        curr_ridge.fit(train_loocv, y_train_loocv)\n",
    "        models.append(curr_ridge)\n",
    "    \n",
    "    for i in range(8):\n",
    "\n",
    "        curr_preds =  models[i].predict(val_loocv)\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "mses = []\n",
    "for i in range(8):\n",
    "    avg_mse = metrics[i] / 5\n",
    "    mses.append(avg_mse)\n",
    "    print(f\"Avg MSE (val data) for ridge regression with lambda={i-2}: {avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.527683601426254\n"
     ]
    }
   ],
   "source": [
    "# lowest lamda is lamda = -2 -> 10^-2 = 0.1\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(train_x, train_y)\n",
    "\n",
    "predictions = lasso.predict(test_x)\n",
    "mse = sm.tools.eval_measures.mse(test_y, predictions)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the estimated coefficients for your lasso to those from the ridge regression -- did lasso result in a more sparse model? (i.e. did it set some coefficients to zero and thus eliminate features from the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients:\n",
      "[-0.         -0.         -0.00812077 -0.00561074 -0.03032004  0.3949268\n",
      "  0.54294944]\n",
      "Ridge coefficients:\n",
      "[-0.36674651  0.00756434 -0.01631279 -0.00536778 -0.07208105  0.41626303\n",
      "  0.90708839]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso coefficients:\")\n",
    "print(lasso.coef_)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_lowest_lamda.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from solutions: \n",
    "\n",
    "Lasso did result in a sparse model, 2 coefficients were set to 0, while in the Ridge model no coefficients were set to 0. the coefficients that were set to 0 lead us to say that those two features are useless in terms of predicting what we would like to predict.\n",
    "\n",
    "last part written by Lamberto Ragnolini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-07-spark-1iOGQZrB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
