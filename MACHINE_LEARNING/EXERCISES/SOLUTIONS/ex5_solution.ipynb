{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5, Machine Learning 2022\n",
    "\n",
    "The following lab-session is adapted from those of Sections 5.3 and 6.3 in Introduction to Statistical Learning with R.\n",
    "\n",
    "### The Auto dataset\n",
    "We use the `Auto` dataset that was used as an example throughout Chapter 3 on linear regression. Here, we treat the variable `mpg` (gas miles in miles per gallon) as the response and `horsepower` as the single predictor.\n",
    "\n",
    "The data has 392 observations on the following 9 variables.\n",
    "\n",
    "`mpg` miles per gallon\n",
    "\n",
    "`cylinders` Number of cylinders between 4 and 8 displacementEngine displacement (cu. inches) \n",
    "\n",
    "`horsepower` Engine horsepower\n",
    "\n",
    "`weight` Vehicle weight (lbs.)\n",
    "\n",
    "`acceleration` Time to accelerate from 0 to 60 mph (sec.)\n",
    "\n",
    "`year` Model year (modulo 100)\n",
    "\n",
    "`origin` Origin of car (1. American, 2. European, 3. Japanese)\n",
    "\n",
    "`name` Vehicle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame\n",
    "from math import log, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0         130    3504          12.0    70   \n",
       "1  15.0          8         350.0         165    3693          11.5    70   \n",
       "2  18.0          8         318.0         150    3436          11.0    70   \n",
       "3  16.0          8         304.0         150    3433          12.0    70   \n",
       "4  17.0          8         302.0         140    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv; change the directory as you need!\n",
    "auto = read_csv(\"Auto.csv\")\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is recommended that you set a random seed, so that any results based on randomness are recreated whenever you run the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set seed!\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation set approach\n",
    "\n",
    "### TO DO\n",
    "\n",
    "- Split the set of observations into two halves by selecting a random subset of 196 obervations out of the original 392 observations. We refer to these observations as the *training set* and the remaining observations as the *validation set*.    \n",
    "- **TIPS**: _For creating the random split, you can use `random.sample(seq, n)` to select `n` numbers without replacement from the sequence `seq`. (There is also a built-in function `train_test_split` that can split your data, but here you split it yourself so that you understand in detail how to achieve the datasets)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_split(n, n_samples):\n",
    "    \n",
    "    train_idx = random.sample([x for x in range(n)], n_samples)\n",
    "\n",
    "    val_idx = [x for x in range(n) if x not in train_idx]\n",
    "    \n",
    "    return train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = randomize_split(392, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = auto.iloc[train_idx, :]\n",
    "X_train = train[\"horsepower\"]\n",
    "X_train = sm.add_constant(X_train) #add column of 1s in the design matrix bc statsmodels doesn't automatically do this\n",
    "y_train = train[\"mpg\"]\n",
    "\n",
    "val = auto.iloc[val_idx, :]\n",
    "X_val = val[\"horsepower\"]\n",
    "X_val = sm.add_constant(X_val)\n",
    "y_val = val[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 2)\n",
      "(196,)\n",
      "(196, 2)\n",
      "(196,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print design matrix to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const  horsepower\n",
      "276    1.0          71\n",
      "148    1.0          93\n",
      "312    1.0          88\n",
      "15     1.0          95\n",
      "318    1.0          92\n",
      "..     ...         ...\n",
      "77     1.0          87\n",
      "355    1.0          76\n",
      "300    1.0          70\n",
      "89     1.0         198\n",
      "38     1.0         175\n",
      "\n",
      "[196 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Fit a linear regression on the training set: `mpg`$= \\beta_0 + \\beta_1$ `horsepower` $+\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Sep 2024</td> <th>  Prob (F-statistic):</th> <td>5.93e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:01:13</td>     <th>  Log-Likelihood:    </th> <td> -593.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   196</td>      <th>  AIC:               </th> <td>   1192.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   194</td>      <th>  BIC:               </th> <td>   1198.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>   39.9738</td> <td>    1.024</td> <td>   39.029</td> <td> 0.000</td> <td>   37.954</td> <td>   41.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th> <td>   -0.1575</td> <td>    0.009</td> <td>  -17.225</td> <td> 0.000</td> <td>   -0.175</td> <td>   -0.139</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.826</td> <th>  Durbin-Watson:     </th> <td>   2.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  16.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.632</td> <th>  Prob(JB):          </th> <td>0.000330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.605</td> <th>  Cond. No.          </th> <td>    319.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.605   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.603   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     296.7   \\\\\n",
       "\\textbf{Date:}             & Tue, 10 Sep 2024 & \\textbf{  Prob (F-statistic):} &  5.93e-41   \\\\\n",
       "\\textbf{Time:}             &     11:01:13     & \\textbf{  Log-Likelihood:    } &   -593.79   \\\\\n",
       "\\textbf{No. Observations:} &         196      & \\textbf{  AIC:               } &     1192.   \\\\\n",
       "\\textbf{Df Residuals:}     &         194      & \\textbf{  BIC:               } &     1198.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}      &      39.9738  &        1.024     &    39.029  &         0.000        &       37.954    &       41.994     \\\\\n",
       "\\textbf{horsepower} &      -0.1575  &        0.009     &   -17.225  &         0.000        &       -0.175    &       -0.139     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 14.826 & \\textbf{  Durbin-Watson:     } &    2.088  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   16.035  \\\\\n",
       "\\textbf{Skew:}          &  0.632 & \\textbf{  Prob(JB):          } & 0.000330  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.605 & \\textbf{  Cond. No.          } &     319.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.605\n",
       "Model:                            OLS   Adj. R-squared:                  0.603\n",
       "Method:                 Least Squares   F-statistic:                     296.7\n",
       "Date:                Tue, 10 Sep 2024   Prob (F-statistic):           5.93e-41\n",
       "Time:                        11:01:13   Log-Likelihood:                -593.79\n",
       "No. Observations:                 196   AIC:                             1192.\n",
       "Df Residuals:                     194   BIC:                             1198.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         39.9738      1.024     39.029      0.000      37.954      41.994\n",
       "horsepower    -0.1575      0.009    -17.225      0.000      -0.175      -0.139\n",
       "==============================================================================\n",
       "Omnibus:                       14.826   Durbin-Watson:                   2.088\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               16.035\n",
       "Skew:                           0.632   Prob(JB):                     0.000330\n",
       "Kurtosis:                       3.605   Cond. No.                         319.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm1 = sm.OLS(y_train, X_train).fit()\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSE on the 196 observations that are the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions on the validation set and calculate the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      16.356248\n",
       "4      17.930753\n",
       "6       5.334711\n",
       "7       6.121964\n",
       "8       4.547459\n",
       "         ...    \n",
       "385    26.747983\n",
       "386    25.803280\n",
       "388    31.786399\n",
       "389    26.747983\n",
       "390    27.535235\n",
       "Length: 196, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val =  lm1.get_prediction(X_val).summary_frame()[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels.tools.eval_measures.mse(x1, x2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.842577966182787"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_val = sm.tools.eval_measures.mse(y_val, pred_val)\n",
    "mse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO \n",
    "- Now estimate also the MSE for the quadradic and cubic regressions.\n",
    "The ISLwR book found an estimated test MSE of (23.27, 18.27, and 18.79) for the three regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting the MSE of a given statsmodels model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(model, X, y):\n",
    "    \n",
    "    preds =  model.get_prediction(X).summary_frame()[\"mean\"]\n",
    "    mse = sm.tools.eval_measures.mse(y, preds)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit quadratic and cubic regressions using statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sm.OLS method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the polynomial features in the design matrix (X) so to do that I will use the PolynomialFeatures class from sklearn to get the design matrices in very few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[\"horsepower\"].to_numpy()\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "X_train_2deg = polynomial_features.fit_transform(X.reshape(-1,1)) #if you don't reshape X as such if it's a single feature\n",
    "                                                                #vector you will get an error telling you to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what the design matrix for the quadratic model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2deg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000e+00, 7.1000e+01, 5.0410e+03],\n",
       "       [1.0000e+00, 9.3000e+01, 8.6490e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 9.2000e+01, 8.4640e+03],\n",
       "       [1.0000e+00, 8.4000e+01, 7.0560e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 1.6000e+02, 2.5600e+04],\n",
       "       [1.0000e+00, 1.8000e+02, 3.2400e+04],\n",
       "       [1.0000e+00, 8.3000e+01, 6.8890e+03],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 2.2500e+02, 5.0625e+04],\n",
       "       [1.0000e+00, 1.6500e+02, 2.7225e+04],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 1.0200e+02, 1.0404e+04],\n",
       "       [1.0000e+00, 8.4000e+01, 7.0560e+03],\n",
       "       [1.0000e+00, 1.1500e+02, 1.3225e+04],\n",
       "       [1.0000e+00, 1.1500e+02, 1.3225e+04],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 9.2000e+01, 8.4640e+03],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 1.4200e+02, 2.0164e+04],\n",
       "       [1.0000e+00, 6.2000e+01, 3.8440e+03],\n",
       "       [1.0000e+00, 6.1000e+01, 3.7210e+03],\n",
       "       [1.0000e+00, 1.4500e+02, 2.1025e+04],\n",
       "       [1.0000e+00, 1.2500e+02, 1.5625e+04],\n",
       "       [1.0000e+00, 8.6000e+01, 7.3960e+03],\n",
       "       [1.0000e+00, 6.8000e+01, 4.6240e+03],\n",
       "       [1.0000e+00, 5.2000e+01, 2.7040e+03],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 1.5500e+02, 2.4025e+04],\n",
       "       [1.0000e+00, 8.0000e+01, 6.4000e+03],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 2.0800e+02, 4.3264e+04],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 1.2000e+02, 1.4400e+04],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 8.6000e+01, 7.3960e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 1.2000e+02, 1.4400e+04],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 1.7000e+02, 2.8900e+04],\n",
       "       [1.0000e+00, 1.5800e+02, 2.4964e+04],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 6.2000e+01, 3.8440e+03],\n",
       "       [1.0000e+00, 1.7500e+02, 3.0625e+04],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 9.6000e+01, 9.2160e+03],\n",
       "       [1.0000e+00, 1.3900e+02, 1.9321e+04],\n",
       "       [1.0000e+00, 7.8000e+01, 6.0840e+03],\n",
       "       [1.0000e+00, 7.1000e+01, 5.0410e+03],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.7000e+02, 2.8900e+04],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 4.8000e+01, 2.3040e+03],\n",
       "       [1.0000e+00, 1.4000e+02, 1.9600e+04],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 7.1000e+01, 5.0410e+03],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 1.3700e+02, 1.8769e+04],\n",
       "       [1.0000e+00, 2.2500e+02, 5.0625e+04],\n",
       "       [1.0000e+00, 1.3200e+02, 1.7424e+04],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 8.0000e+01, 6.4000e+03],\n",
       "       [1.0000e+00, 2.3000e+02, 5.2900e+04],\n",
       "       [1.0000e+00, 9.4000e+01, 8.8360e+03],\n",
       "       [1.0000e+00, 5.2000e+01, 2.7040e+03],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 1.4500e+02, 2.1025e+04],\n",
       "       [1.0000e+00, 7.2000e+01, 5.1840e+03],\n",
       "       [1.0000e+00, 1.6500e+02, 2.7225e+04],\n",
       "       [1.0000e+00, 6.0000e+01, 3.6000e+03],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 7.2000e+01, 5.1840e+03],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 8.1000e+01, 6.5610e+03],\n",
       "       [1.0000e+00, 8.0000e+01, 6.4000e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 6.4000e+01, 4.0960e+03],\n",
       "       [1.0000e+00, 6.8000e+01, 4.6240e+03],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 8.5000e+01, 7.2250e+03],\n",
       "       [1.0000e+00, 1.2900e+02, 1.6641e+04],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 1.6500e+02, 2.7225e+04],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 9.7000e+01, 9.4090e+03],\n",
       "       [1.0000e+00, 7.4000e+01, 5.4760e+03],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 1.0300e+02, 1.0609e+04],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 9.8000e+01, 9.6040e+03],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 1.4000e+02, 1.9600e+04],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 6.8000e+01, 4.6240e+03],\n",
       "       [1.0000e+00, 7.1000e+01, 5.0410e+03],\n",
       "       [1.0000e+00, 6.7000e+01, 4.4890e+03],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 6.3000e+01, 3.9690e+03],\n",
       "       [1.0000e+00, 4.8000e+01, 2.3040e+03],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.0800e+02, 1.1664e+04],\n",
       "       [1.0000e+00, 6.6000e+01, 4.3560e+03],\n",
       "       [1.0000e+00, 1.6500e+02, 2.7225e+04],\n",
       "       [1.0000e+00, 1.3300e+02, 1.7689e+04],\n",
       "       [1.0000e+00, 9.6000e+01, 9.2160e+03],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 8.5000e+01, 7.2250e+03],\n",
       "       [1.0000e+00, 1.0500e+02, 1.1025e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 8.5000e+01, 7.2250e+03],\n",
       "       [1.0000e+00, 7.6000e+01, 5.7760e+03],\n",
       "       [1.0000e+00, 5.3000e+01, 2.8090e+03],\n",
       "       [1.0000e+00, 1.8000e+02, 3.2400e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 8.2000e+01, 6.7240e+03],\n",
       "       [1.0000e+00, 1.4900e+02, 2.2201e+04],\n",
       "       [1.0000e+00, 9.2000e+01, 8.4640e+03],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 1.3000e+02, 1.6900e+04],\n",
       "       [1.0000e+00, 1.2200e+02, 1.4884e+04],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 1.4500e+02, 2.1025e+04],\n",
       "       [1.0000e+00, 8.3000e+01, 6.8890e+03],\n",
       "       [1.0000e+00, 2.1500e+02, 4.6225e+04],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 7.7000e+01, 5.9290e+03],\n",
       "       [1.0000e+00, 8.6000e+01, 7.3960e+03],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 7.2000e+01, 5.1840e+03],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 1.7500e+02, 3.0625e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 1.8000e+02, 3.2400e+04],\n",
       "       [1.0000e+00, 1.4000e+02, 1.9600e+04],\n",
       "       [1.0000e+00, 2.1000e+02, 4.4100e+04],\n",
       "       [1.0000e+00, 9.0000e+01, 8.1000e+03],\n",
       "       [1.0000e+00, 7.6000e+01, 5.7760e+03],\n",
       "       [1.0000e+00, 8.9000e+01, 7.9210e+03],\n",
       "       [1.0000e+00, 6.9000e+01, 4.7610e+03],\n",
       "       [1.0000e+00, 1.1200e+02, 1.2544e+04],\n",
       "       [1.0000e+00, 1.0000e+02, 1.0000e+04],\n",
       "       [1.0000e+00, 4.6000e+01, 2.1160e+03],\n",
       "       [1.0000e+00, 8.8000e+01, 7.7440e+03],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 5.3000e+01, 2.8090e+03],\n",
       "       [1.0000e+00, 1.1000e+02, 1.2100e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 1.9800e+02, 3.9204e+04],\n",
       "       [1.0000e+00, 9.5000e+01, 9.0250e+03],\n",
       "       [1.0000e+00, 6.5000e+01, 4.2250e+03],\n",
       "       [1.0000e+00, 7.5000e+01, 5.6250e+03],\n",
       "       [1.0000e+00, 1.7000e+02, 2.8900e+04],\n",
       "       [1.0000e+00, 8.5000e+01, 7.2250e+03],\n",
       "       [1.0000e+00, 7.8000e+01, 6.0840e+03],\n",
       "       [1.0000e+00, 1.4000e+02, 1.9600e+04],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 1.5000e+02, 2.2500e+04],\n",
       "       [1.0000e+00, 8.7000e+01, 7.5690e+03],\n",
       "       [1.0000e+00, 7.6000e+01, 5.7760e+03],\n",
       "       [1.0000e+00, 7.0000e+01, 4.9000e+03],\n",
       "       [1.0000e+00, 1.9800e+02, 3.9204e+04],\n",
       "       [1.0000e+00, 1.7500e+02, 3.0625e+04]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the actual quadratic model and calculate the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then do the same for the cubic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   222.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Sep 2024</td> <th>  Prob (F-statistic):</th> <td>7.60e-51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:16:40</td>     <th>  Log-Likelihood:    </th> <td> -567.53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   196</td>      <th>  AIC:               </th> <td>   1141.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   193</td>      <th>  BIC:               </th> <td>   1151.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   58.1407</td> <td>    2.524</td> <td>   23.032</td> <td> 0.000</td> <td>   53.162</td> <td>   63.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.4861</td> <td>    0.043</td> <td>  -11.194</td> <td> 0.000</td> <td>   -0.572</td> <td>   -0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0013</td> <td>    0.000</td> <td>    7.701</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>17.593</td> <th>  Durbin-Watson:     </th> <td>   2.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  35.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.417</td> <th>  Prob(JB):          </th> <td>1.99e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.910</td> <th>  Cond. No.          </th> <td>1.29e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.29e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.698   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.694   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     222.6   \\\\\n",
       "\\textbf{Date:}             & Tue, 10 Sep 2024 & \\textbf{  Prob (F-statistic):} &  7.60e-51   \\\\\n",
       "\\textbf{Time:}             &     11:16:40     & \\textbf{  Log-Likelihood:    } &   -567.53   \\\\\n",
       "\\textbf{No. Observations:} &         196      & \\textbf{  AIC:               } &     1141.   \\\\\n",
       "\\textbf{Df Residuals:}     &         193      & \\textbf{  BIC:               } &     1151.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      58.1407  &        2.524     &    23.032  &         0.000        &       53.162    &       63.120     \\\\\n",
       "\\textbf{x1}    &      -0.4861  &        0.043     &   -11.194  &         0.000        &       -0.572    &       -0.400     \\\\\n",
       "\\textbf{x2}    &       0.0013  &        0.000     &     7.701  &         0.000        &        0.001    &        0.002     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 17.593 & \\textbf{  Durbin-Watson:     } &    2.224  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   35.470  \\\\\n",
       "\\textbf{Skew:}          &  0.417 & \\textbf{  Prob(JB):          } & 1.99e-08  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.910 & \\textbf{  Cond. No.          } & 1.29e+05  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.29e+05. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.694\n",
       "Method:                 Least Squares   F-statistic:                     222.6\n",
       "Date:                Tue, 10 Sep 2024   Prob (F-statistic):           7.60e-51\n",
       "Time:                        11:16:40   Log-Likelihood:                -567.53\n",
       "No. Observations:                 196   AIC:                             1141.\n",
       "Df Residuals:                     193   BIC:                             1151.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         58.1407      2.524     23.032      0.000      53.162      63.120\n",
       "x1            -0.4861      0.043    -11.194      0.000      -0.572      -0.400\n",
       "x2             0.0013      0.000      7.701      0.000       0.001       0.002\n",
       "==============================================================================\n",
       "Omnibus:                       17.593   Durbin-Watson:                   2.224\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               35.470\n",
       "Skew:                           0.417   Prob(JB):                     1.99e-08\n",
       "Kurtosis:                       4.910   Cond. No.                     1.29e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.29e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_quad = sm.OLS(y_train, X_train_2deg).fit()\n",
    "sm_quad.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.852745430127555\n"
     ]
    }
   ],
   "source": [
    "X_val_2deg = polynomial_features.fit_transform((val[\"horsepower\"]).to_numpy().reshape(-1,1))\n",
    "mse_quad = get_mse(sm_quad, X_val_2deg, y_val)\n",
    "print(mse_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for the cubic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.075536759324212\n"
     ]
    }
   ],
   "source": [
    "poly3 = PolynomialFeatures(degree=3)\n",
    "\n",
    "X_train_3deg = poly3.fit_transform((train[\"horsepower\"]).to_numpy().reshape(-1,1))\n",
    "X_val_3deg = poly3.fit_transform((val[\"horsepower\"]).to_numpy().reshape(-1,1))\n",
    "\n",
    "sm_cube = sm.OLS(y_train, X_train_3deg).fit()\n",
    "\n",
    "mse_cube = get_mse(sm_cube, X_val_3deg, y_val)\n",
    "\n",
    "print(mse_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formula api (smf.ols) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also achieve the same with the formula api**  \n",
    "**Example with the cubic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.075536759324212"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_cube = smf.ols(\"mpg ~ 1+ horsepower + I(horsepower**2) + I(horsepower**3)\", train).fit()\n",
    "pred_val_cube =  lm_cube.get_prediction(val).summary_frame()[\"mean\"]\n",
    "mse_val_cube = sm.tools.eval_measures.mse(y_val, pred_val_cube)\n",
    "mse_val_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Set another seed and create a different split of your data - do you get different results? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.596948801393975\n",
      "20.14035992764117\n",
      "20.77892944825929\n"
     ]
    }
   ],
   "source": [
    "random.seed(22)\n",
    "\n",
    "train_idx2, val_idx2 = randomize_split(392, 196)\n",
    "\n",
    "train2 = auto.iloc[train_idx2, :]\n",
    "val2 = auto.iloc[val_idx2, :]\n",
    "true_val2 = val2[\"mpg\"]\n",
    "val2 = val2[\"horsepower\"]\n",
    "\n",
    "lm12 = smf.ols(\"mpg ~ 1+ horsepower\", train2).fit()\n",
    "mse_val2 = get_mse(lm12, val2, true_val2)\n",
    "print(mse_val2)\n",
    "\n",
    "lm_quad2 = smf.ols(\"mpg ~ 1+ horsepower + I(horsepower**2)\", train2).fit()\n",
    "mse_val_quad2 = get_mse(lm_quad2, val2, true_val2)\n",
    "print(mse_val_quad2)\n",
    "\n",
    "lm_cube2 = smf.ols(\"mpg ~ 1+ horsepower + I(horsepower**2) + I(horsepower**3)\", train2).fit()\n",
    "mse_val_cube2 = get_mse(lm_cube2, val2, true_val2)\n",
    "print(mse_val_cube2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do get different results because we are using different data for the training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Which model would you choose based on the estimated MSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would choose the 2nd model (the quadratic one), because it has the lowest MSE, while still being a fairly simple model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- If you instead train the models on the entire data and perform model selection via F-tests or AIC, would you choose differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1_3 = smf.ols(\"mpg ~ 1+ horsepower\", auto).fit()\n",
    "lm_quad_3 = smf.ols(\"mpg ~ 1+ horsepower + I(horsepower**2)\", auto).fit()\n",
    "lm_cube_3 = smf.ols(\"mpg ~ 1+ horsepower + I(horsepower**2) + I(horsepower**3)\", auto).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361.3236578374017\n",
      "2272.353522359692\n",
      "2273.5312967115547\n"
     ]
    }
   ],
   "source": [
    "print(lm1_3.aic)\n",
    "print(lm_quad_3.aic)\n",
    "print(lm_cube_3.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on AIC only, I would choose the second model because the general rule of thumb is the lower AIC the better. However this metric is known to favoritize simpler models, and using only using AIC to choose may result in underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross-validation\n",
    "\n",
    "### TO DO\n",
    "- Implement leave-one-out cross-validation and use it to estimate the MSE for the three regression models above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Here I will use the class from sklearn, but you can try to implement it from scratch yourself as an extra challenge if you're feeling brave :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to format the polynomial formula for the formula api\n",
    "I am doing this because we will need it later on and we can use it from now to make the code a lot nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_formula(d):\n",
    "    \n",
    "    f = \"mpg ~ 1 + horsepower\"\n",
    "    \n",
    "    if d == 1:\n",
    "        return f\n",
    "    else:\n",
    "        for i in range(2, d+1):\n",
    "            f += f\" + I(horsepower**{i})\"\n",
    "   \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mpg ~ 1 + horsepower + I(horsepower**2) + I(horsepower**3) + I(horsepower**4) + I(horsepower**5)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_formula(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate MSE for the 3 models using LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.231513517929244\n",
      "19.248213124489315\n",
      "19.334984064060897\n"
     ]
    }
   ],
   "source": [
    "metrics = [0] * 3\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(loocv.split(auto)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = auto.iloc[train_idx]\n",
    "    val_loocv = auto.iloc[val_idx]\n",
    "    true_vals = val_loocv[\"mpg\"]\n",
    "    val_loocv = val_loocv[\"horsepower\"]\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        formula = format_formula(i)\n",
    "        curr_lm = smf.ols(formula, train_loocv).fit()\n",
    "        models.append(curr_lm)\n",
    "    \n",
    "    for i in range(3):\n",
    "\n",
    "        curr_preds =  models[i].get_prediction(val_loocv).summary_frame()[\"mean\"]\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "for i in range(3):\n",
    "    avg_mse = metrics[i] / (auto.shape[0])\n",
    "    print(avg_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation\n",
    "\n",
    "### TO DO\n",
    "- Estimate the MSE now from k-fold cross-validation that you implement yourself. If you set k=10, then you get Figure 5.6 from the book, but you are welcome to choose another k. Make sure that you properly randomise observations into the k folds! One way is to first make a random permutation of row indices and then chop the re-ordered dataset into k (roughly) even parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate of the MSE can also be computed automatially with sklearn. In fact, sklearn provides a lot of functionality for cross-validation. However, our objective right now is to understand how the algorithms work, so that you learn to reflect on them and use the automatic functionality appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Here I have solved the exercise using the KFold class from sklearn. If you want, you can implement KFold yourself as an extra challenge :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "kf = KFold(n_splits = N_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.13816500046387\n",
      "19.208462214531252\n",
      "19.321071228938443\n"
     ]
    }
   ],
   "source": [
    "metrics = [0] * 3\n",
    "\n",
    "auto_sh = auto.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(auto_sh)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = auto_sh.iloc[train_idx]\n",
    "    val_loocv = auto_sh.iloc[val_idx]\n",
    "    true_vals = val_loocv[\"mpg\"]\n",
    "    val_loocv = val_loocv[\"horsepower\"]\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        formula = format_formula(i)\n",
    "        curr_lm = smf.ols(formula, train_loocv).fit()\n",
    "        models.append(curr_lm)\n",
    "    \n",
    "    for i in range(3):\n",
    "\n",
    "        curr_preds =  models[i].get_prediction(val_loocv).summary_frame()[\"mean\"]\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "        \n",
    "for i in range(3):\n",
    "    avg_mse = metrics[i] / N_SPLITS\n",
    "    print(avg_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Extend your code with a for-loop that iterates over increasing orders of the polynomial (1 to 10). Note that is possible to autogenerate a design matrix with all the terms of the polynomial with functions such as `sklearn.preprocessing.PolynomialFeatures` (It should also work for several features; a degree 2 polynomial in features x1 and x2 would need  $1+x_1+x_2+x_1x_2+x_1^2+x_2^2$, i.e. also the term $x_1x_2$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE for poly d=1: 24.13816500046387\n",
      "Avg MSE for poly d=2: 19.208462214531252\n",
      "Avg MSE for poly d=3: 19.321071228938443\n",
      "Avg MSE for poly d=4: 19.40890138329201\n",
      "Avg MSE for poly d=5: 19.051356330681738\n",
      "Avg MSE for poly d=6: 19.084930326160457\n",
      "Avg MSE for poly d=7: 19.09371772140657\n",
      "Avg MSE for poly d=8: 23.15076714068223\n",
      "Avg MSE for poly d=9: 23.228094568327943\n",
      "Avg MSE for poly d=10: 51.32054181793635\n"
     ]
    }
   ],
   "source": [
    "metrics = [0] * 10;\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(auto_sh)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = auto_sh.iloc[train_idx]\n",
    "    val_loocv = auto_sh.iloc[val_idx]\n",
    "    true_vals = val_loocv[\"mpg\"]\n",
    "    val_loocv = val_loocv[\"horsepower\"]\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        \n",
    "        formula = format_formula(i)\n",
    "        curr_lm = smf.ols(formula, train_loocv).fit()\n",
    "        models.append(curr_lm)\n",
    "    \n",
    "    for i in range(10):\n",
    "\n",
    "        curr_preds =  models[i].get_prediction(val_loocv).summary_frame()[\"mean\"]\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "mses = []\n",
    "for i in range(10):\n",
    "    avg_mse = metrics[i] / 10\n",
    "    mses.append(avg_mse)\n",
    "    print(f\"Avg MSE for poly d={i+1}: {avg_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "- Plot the resulting MSE curve against the degree of the polynomial (this is the complexity or the flexibility).   \n",
    "- Do you get the same conclusion as before regarding your choice of model?   \n",
    "- Was it faster than LOOCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(1,11)), mses)\n",
    "plt.title(\"Average MSE for polynomial regression of different degrees\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Avg. MSE\")\n",
    "plt.xticks(list(range(1,11)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plot, we would still choose the 2nd degree polynomial.\n",
    "We can also notice that the KFold with 10 splits executed a lot faster than the LOOCV (Which makes sense, because LOOCV is esentially KFold with K=n, where n is the number of samples in your dataset. These numbers of folds correspond with how many times you execute the loop, so the more folds you have the slower the calculation will be)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage/Regularization\n",
    "\n",
    "Now try the two shrinkage methods we saw on Thursday, ridge regression and lasso. To make things a bit more exciting, include in your models all variables available in the dataset, and perhaps a few polynomial terms for `horsepower` now that you know they are relevant. The `name` variable is evidently an identifyer and should not be used as a predictor in the model.\n",
    "\n",
    "The Lasso estimates for coefficients $\\beta$ in a linear regression model are found by minimising $RSS + \\lambda\\sum_{j=1}^p|\\beta_j|$ rather than RSS. \n",
    "\n",
    "In Ridge regression, we estimate $\\beta$ by minimising $RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$ rather than RSS. \n",
    "\n",
    "In both cases, we need to set the regularation parameter $\\lambda$. **The parameter $\\lambda$ is called `alpha` in the Python implementations**\n",
    "\n",
    "\n",
    "The relevant method in the statsmodel library is `fit_regularized`, which you use in place of `fit` after instantiating an ols model. If `L1_wt=1`, the fit is the lasso (L1-penalty). Ridge regression (L2-penalty) is obtained by setting `L1_wt=0`.  The reason we need to set parameter `L1_wt` is that the method actually implements the more general 'elastic net' penalty, which is a convex combination of the Lasso penalty and the ridge regression penalty with weight `L1_wt`. The fitting method returns a `RegressionResults` object, from which you can extract `params` and `fittedvalues`, and it has a `predict` method just like the usual linear regression output from ols.\n",
    "\n",
    "The sklearn libary has methods `Ridge` and `Lasso`. \n",
    "\n",
    "**Set aside a test set of about 25% of the data that you can use at the end to estimate the MSE of your final model. Use the remaining data to train the regularised regression models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "### TO DO\n",
    "- First, carry out a ridge regression where the regulation parameter $\\lambda$ is fixed at 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 7)\n",
      "(294,)\n",
      "(98, 7)\n",
      "(98,)\n"
     ]
    }
   ],
   "source": [
    "n_train = int(0.75 * (auto.shape[0]))\n",
    "X_train = (auto.iloc[:n_train, 1:8]).to_numpy()\n",
    "y_train = (auto.iloc[:n_train, 0]).to_numpy()\n",
    "X_test = (auto.iloc[n_train:, 1:8]).to_numpy()\n",
    "y_test = (auto.iloc[n_train:, 0]).to_numpy()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Ridge<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html\">?<span>Documentation for Ridge</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Ridge(alpha=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = Ridge(alpha=10)\n",
    "ridge_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.86250991322604\n"
     ]
    }
   ],
   "source": [
    "preds_ridge = ridge_reg.predict(X_test)\n",
    "mse_ridge = sm.tools.eval_measures.mse(y_test, preds_ridge)\n",
    "print(mse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, $\\lambda = 10$ is a rather arbitrary suggestion. We need to decide on a suitable value for the hyperparameter $\\lambda$. This we can do using a dedicated validation set, or we could do it by cross-validation. We do the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Take a suitable range of $\\lambda \\in [0, \\infty)$, for instance you can take a sequence $10^i, i=-2, \\ldots, 5$. Using 5-fold cross-validation to estimate the MSE (call it `MSE_lambda`) for each $\\lambda$, make a plot of how MSE changes with $\\lambda$. Based on this plot, how would you choose $\\lambda$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE (val data) for ridge regression with lambda=-2: 8.253391197794063\n",
      "Avg MSE (val data) for ridge regression with lambda=-1: 8.253161868693871\n",
      "Avg MSE (val data) for ridge regression with lambda=0: 8.250969096054973\n",
      "Avg MSE (val data) for ridge regression with lambda=1: 8.23674448389388\n",
      "Avg MSE (val data) for ridge regression with lambda=2: 8.244015096355735\n",
      "Avg MSE (val data) for ridge regression with lambda=3: 8.50045245146602\n",
      "Avg MSE (val data) for ridge regression with lambda=4: 9.49629757132169\n",
      "Avg MSE (val data) for ridge regression with lambda=5: 9.773769993769728\n"
     ]
    }
   ],
   "source": [
    "train_data = auto_sh.iloc[:n_train]\n",
    "\n",
    "kf5 = KFold(n_splits=5)\n",
    "metrics = [0] * 8;\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf5.split(train_data)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = train_data.iloc[train_idx]\n",
    "    y_train_loocv = train_loocv.iloc[:,0]\n",
    "    train_loocv = train_loocv.iloc[:,1:8]\n",
    "    val_loocv = train_data.iloc[val_idx]\n",
    "    true_vals = val_loocv.iloc[:, 0]\n",
    "    val_loocv = val_loocv.iloc[:, 1:8]\n",
    "    \n",
    "    for i in range(-2, 6):\n",
    "        \n",
    "        curr_lambda = 10**i\n",
    "        curr_ridge = Ridge(alpha = curr_lambda)\n",
    "        curr_ridge.fit(train_loocv, y_train_loocv)\n",
    "        models.append(curr_ridge)\n",
    "    \n",
    "    for i in range(8):\n",
    "\n",
    "        curr_preds =  models[i].predict(val_loocv)\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "mses = []\n",
    "for i in range(8):\n",
    "    avg_mse = metrics[i] / 5\n",
    "    mses.append(avg_mse)\n",
    "    print(f\"Avg MSE (val data) for ridge regression with lambda={i-2}: {avg_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Having chosen a $\\lambda$, would it be appropriate to report the corresponding `MSE_lambda` as the test error for the ridge regression?\n",
    "- Compute the test MSE using the dedicated test set and compare to `MSE_lambda`.  \n",
    "- In fact, once we have chosen the regularisation parameter $\\lambda$, we should preferably make use of *all of the data* to (re-)train the model, and then report the associated test MSE from the test data. Do this, and inspect the model estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.51363598342477\n"
     ]
    }
   ],
   "source": [
    "ridge1 = Ridge(alpha=1)\n",
    "ridge1.fit(X_train, y_train)\n",
    "\n",
    "preds_ridge = ridge1.predict(X_test)\n",
    "mse_ridge = sm.tools.eval_measures.mse(y_test, preds_ridge)\n",
    "print(mse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "### TO DO\n",
    "- Following the same procedure as for ridge regression, train a lasso regression where the regularisation parameter $\\lambda$ is selected by cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE (val data) for ridge regression with lambda=-2: 8.248425641444753\n",
      "Avg MSE (val data) for ridge regression with lambda=-1: 8.278748431381159\n",
      "Avg MSE (val data) for ridge regression with lambda=0: 8.64271030117902\n",
      "Avg MSE (val data) for ridge regression with lambda=1: 9.935480758271392\n",
      "Avg MSE (val data) for ridge regression with lambda=2: 9.851834565525374\n",
      "Avg MSE (val data) for ridge regression with lambda=3: 11.15619058530283\n",
      "Avg MSE (val data) for ridge regression with lambda=4: 42.34772478177712\n",
      "Avg MSE (val data) for ridge regression with lambda=5: 42.34772478177712\n"
     ]
    }
   ],
   "source": [
    "train_data = auto_sh.iloc[:n_train]\n",
    "\n",
    "kf5 = KFold(n_splits=5)\n",
    "metrics = [0] * 8;\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf5.split(train_data)):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    train_loocv = train_data.iloc[train_idx]\n",
    "    y_train_loocv = train_loocv.iloc[:,0]\n",
    "    train_loocv = train_loocv.iloc[:,1:8]\n",
    "    val_loocv = train_data.iloc[val_idx]\n",
    "    true_vals = val_loocv.iloc[:, 0]\n",
    "    val_loocv = val_loocv.iloc[:, 1:8]\n",
    "    \n",
    "    for i in range(-2, 6):\n",
    "        \n",
    "        curr_lambda = 10**i\n",
    "        curr_ridge = Lasso(alpha = curr_lambda)\n",
    "        curr_ridge.fit(train_loocv, y_train_loocv)\n",
    "        models.append(curr_ridge)\n",
    "    \n",
    "    for i in range(8):\n",
    "\n",
    "        curr_preds =  models[i].predict(val_loocv)\n",
    "        metrics[i] += sm.tools.eval_measures.mse(true_vals, curr_preds)\n",
    "\n",
    "mses = []\n",
    "for i in range(8):\n",
    "    avg_mse = metrics[i] / 5\n",
    "    mses.append(avg_mse)\n",
    "    print(f\"Avg MSE (val data) for ridge regression with lambda={i-2}: {avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.52768360142623\n"
     ]
    }
   ],
   "source": [
    "lasso1 = Lasso(alpha=0.1)\n",
    "lasso1.fit(X_train, y_train)\n",
    "\n",
    "preds_lasso = lasso1.predict(X_test)\n",
    "mse_lasso = sm.tools.eval_measures.mse(y_test, preds_lasso)\n",
    "print(mse_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients:\n",
      "[-0.         -0.         -0.00812077 -0.00561074 -0.03032004  0.3949268\n",
      "  0.54294944]\n",
      "Ridge coefficients:\n",
      "[-0.36674651  0.00756434 -0.01631279 -0.00536778 -0.07208105  0.41626303\n",
      "  0.90708839]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso coefficients:\")\n",
    "print(lasso1.coef_)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Compare the estimated coefficients for your lasso to those from the ridge regression -- did lasso result in a more sparse model? (i.e. did it set some coefficients to zero and thus eliminate features from the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso did result in a sparse model, 2 coefficients were set to 0, while in the Ridge model no coefficients were set to 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-07-spark-1iOGQZrB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
